{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device Research Artifact - Experimental Evaluation\n",
        "\n",
        "This notebook contains all experiments from the MICRO'25 paper _Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device_. Each section corresponds to one experiment that can be run independently or as part of the complete evaluation.\n",
        "\n",
        "## Overview of Experiments\n",
        "\n",
        "1. **Binary Matrix Multiplication (1-bmatmul)**: Performance breakdown analysis of binary matrix multiplication with different optimization levels\n",
        "2. **Phoenix Benchmark Suite (2-phoenix)**: Speedup evaluation across multiple benchmarks comparing CPU, GPU, and APU implementations\n",
        "3. **Analytical Model Validation (3-analytical)**: Validation of analytical performance models against measured results\n",
        "4. **RAG End-to-End Inference (4-rag-e2e)**: End-to-end inference time analysis for Retrieval-Augmented Generation workloads\n",
        "5. **RAG Energy Analysis (5-rag-energy)**: Energy consumption comparison between GPU and compute-in-SRAM approaches\n",
        "6. **RAG Latency Breakdown (6-rag-latency-breakdown)**: Detailed latency breakdown analysis for RAG components\n",
        "\n",
        "## Instructions\n",
        "\n",
        "- Run individual experiment cells to execute specific experiments\n",
        "- Run all cells to execute the complete evaluation suite\n",
        "- Some experiments produce PDF figures that will be saved to their respective directories\n",
        "- Terminal outputs will be displayed inline in the notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the absolute path of the artifact root directory\n",
        "artifact_root = Path.cwd().absolute()\n",
        "print(f\"Artifact root directory: {artifact_root}\")\n",
        "\n",
        "# Mapping of experiments to their expected output PNG files\n",
        "EXPERIMENT_FIGURES = {\n",
        "    \"1-bmatmul\": \"bmatmul.png\",\n",
        "    \"2-phoenix\": \"phoenix-speedup.png\", \n",
        "    \"4-rag-e2e\": \"e2e_inference_time.png\",\n",
        "    \"5-rag-energy\": \"energy_comparison.png\"\n",
        "}\n",
        "\n",
        "def display_experiment_figure(experiment_dir):\n",
        "    \"\"\"Display the PNG figure if it exists for this experiment\"\"\"\n",
        "    if experiment_dir in EXPERIMENT_FIGURES:\n",
        "        png_file = EXPERIMENT_FIGURES[experiment_dir]\n",
        "        exp_path = artifact_root / experiment_dir\n",
        "        png_path = exp_path / png_file\n",
        "        \n",
        "        if png_path.exists():\n",
        "            print(f\"\\nüìä Generated Figure:\")\n",
        "            display(Image(filename=str(png_path)))\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Expected figure {png_file} not found in {experiment_dir}\")\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Function to run experiment in its directory\n",
        "def run_experiment(experiment_dir, description=\"\"):\n",
        "    \"\"\"Run an experiment in its specific directory and return to root\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üöÄ Running Experiment: {experiment_dir}\")\n",
        "    if description:\n",
        "        print(f\"üìù Description: {description}\")\n",
        "    print('='*80)\n",
        "    \n",
        "    # Change to experiment directory\n",
        "    original_dir = os.getcwd()\n",
        "    exp_path = artifact_root / experiment_dir\n",
        "    \n",
        "    if not exp_path.exists():\n",
        "        print(f\"‚ùå Error: Directory {experiment_dir} not found!\")\n",
        "        return False\n",
        "    \n",
        "    os.chdir(exp_path)\n",
        "    print(f\"üìÅ Changed to directory: {exp_path}\")\n",
        "    \n",
        "    try:\n",
        "        # Run the experiment\n",
        "        process = subprocess.Popen(\n",
        "            [sys.executable, \"-u\", \"run.py\"],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True,\n",
        "            bufsize=1\n",
        "        )\n",
        "        \n",
        "        # Stream and filter\n",
        "        for line in process.stdout:\n",
        "            if \"E: No packages found\" in line:\n",
        "                continue  # filter it out\n",
        "            print(line, end=\"\")  # preserve real-time behavior\n",
        "        \n",
        "        process.wait()\n",
        "        \n",
        "        if process.returncode == 0:\n",
        "            print(f\"‚úÖ Experiment {experiment_dir} completed successfully!\")\n",
        "            success = True\n",
        "        else:\n",
        "            print(f\"‚ùå Experiment {experiment_dir} failed with return code {result.returncode}\")\n",
        "            success = False\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error running experiment {experiment_dir}: {e}\")\n",
        "        success = False\n",
        "        \n",
        "    finally:\n",
        "        # Always return to original directory\n",
        "        os.chdir(original_dir)\n",
        "    \n",
        "    # Display figure if experiment was successful and produces one\n",
        "    if success:\n",
        "        display_experiment_figure(experiment_dir)\n",
        "        \n",
        "    return success\n",
        "\n",
        "print(\"üîß Setup completed. Ready to run experiments!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Experiment 1: Binary Matrix Multiplication Performance Analysis\n",
        "\n",
        "This experiment evaluates the performance of binary matrix multiplication across different optimization levels on the APU architecture. It runs five configurations:\n",
        "\n",
        "- **Baseline**: No optimizations\n",
        "- **Opt1**: First optimization level  \n",
        "- **Opt2**: Second optimization level\n",
        "- **Opt3**: Third optimization level\n",
        "- **Optimized**: All optimizations combined\n",
        "\n",
        "The experiment produces a performance breakdown chart showing the execution time distribution across different components:\n",
        "- **LD LHS**: Left-hand side matrix loading time\n",
        "- **LD RHS**: Right-hand side matrix loading time  \n",
        "- **VR Op**: Vector register operations time\n",
        "- **ST**: Store operations time\n",
        "\n",
        "**Output**: \n",
        "- Generates `bmatmul.pdf` and `bmatmul.png` with stacked bar chart showing performance breakdown\n",
        "- PNG figure displayed inline below after execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Binary Matrix Multiplication Experiment\n",
        "run_experiment(\"1-bmatmul\", \n",
        "               \"Binary matrix multiplication performance breakdown across optimization levels\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Experiment 2: Phoenix Benchmark Suite Evaluation\n",
        "\n",
        "This experiment runs the Phoenix benchmark suite to compare performance across different computing platforms and optimization levels. It evaluates seven benchmarks:\n",
        "\n",
        "- **Histogram**: Data frequency analysis\n",
        "- **Linear Regression**: Statistical modeling  \n",
        "- **Matrix Multiply**: Dense matrix operations\n",
        "- **K-means**: Clustering algorithm\n",
        "- **Reverse Index**: Text processing\n",
        "- **String Match**: Pattern matching\n",
        "- **Word Count**: Text analysis\n",
        "\n",
        "The experiment compares performance across platforms:\n",
        "- **CPU single-thread**: Baseline single-threaded CPU execution\n",
        "- **CPU multi-thread**: Multi-threaded CPU execution\n",
        "- **APU configurations**: No optimization, Opt1, Opt2, Opt3, and all optimizations\n",
        "\n",
        "**Output**: \n",
        "- Generates `phoenix-speedup.pdf` and `phoenix-speedup.png` with speedup comparison chart\n",
        "- Produces `ablation.json` with detailed performance data\n",
        "- Terminal output showing performance statistics\n",
        "- PNG figure displayed inline below after execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Phoenix Benchmark Suite Experiment  \n",
        "run_experiment(\"2-phoenix\",\n",
        "               \"Phoenix benchmark suite speedup evaluation across CPU, GPU, and APU platforms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Experiment 3: Analytical Model Validation\n",
        "\n",
        "This experiment validates the accuracy of analytical performance models by comparing predicted latencies against actual measured results from the Phoenix benchmark suite. It evaluates the analytical models for all seven Phoenix benchmarks.\n",
        "\n",
        "The experiment:\n",
        "1. **Extracts measured latencies** from the optimized Phoenix benchmark results (from Experiment 2)\n",
        "2. **Runs analytical prediction scripts** for each benchmark  \n",
        "3. **Compares predicted vs. measured values** to calculate error percentages\n",
        "4. **Reports overall model accuracy** across all benchmarks\n",
        "\n",
        "For each benchmark, the analytical model considers:\n",
        "- Memory access patterns and latencies\n",
        "- Compute operation costs  \n",
        "- Data movement overheads\n",
        "- APU architecture-specific optimizations\n",
        "\n",
        "**Output**: \n",
        "- Terminal table showing measured vs. predicted latencies with error percentages\n",
        "- Overall error rate calculation across all benchmarks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Analytical Model Validation Experiment\n",
        "run_experiment(\"3-analytical\", \n",
        "               \"Validation of analytical performance models against measured Phoenix benchmark results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Experiment 4: RAG End-to-End Inference Analysis\n",
        "\n",
        "This experiment evaluates end-to-end inference time for Retrieval-Augmented Generation (RAG) workloads across different platforms and corpus sizes. RAG combines retrieval of relevant documents with language model generation.\n",
        "\n",
        "The experiment tests three corpus sizes:\n",
        "- **10GB**: Small corpus for lightweight workloads\n",
        "- **50GB**: Medium corpus for moderate workloads  \n",
        "- **200GB**: Large corpus for enterprise-scale workloads\n",
        "\n",
        "Platforms evaluated:\n",
        "- **CPU**: Traditional CPU-based retrieval\n",
        "- **GPU**: GPU-accelerated retrieval\n",
        "- **In-SRAM configurations**: APU with different optimization levels (No Opt, Opt1, Opt2, Opt3, All Opts)\n",
        "\n",
        "Components measured:\n",
        "- **Generation**: Language model inference time (consistent across platforms)\n",
        "- **Retrieval**: Document retrieval and similarity computation time (varies by platform)\n",
        "\n",
        "**Output**:\n",
        "- Generates `e2e_inference_time.pdf` and `e2e_inference_time.png` showing time-to-interactive comparison\n",
        "- Terminal output with speedup analysis and performance data\n",
        "- Detailed timing breakdown for each configuration\n",
        "- PNG figure displayed inline below after execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run RAG End-to-End Inference Experiment\n",
        "run_experiment(\"4-rag-e2e\",\n",
        "               \"RAG end-to-end inference time analysis across platforms and corpus sizes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Experiment 5: RAG Energy Consumption Analysis\n",
        "\n",
        "This experiment analyzes the energy consumption of RAG workloads, comparing compute-in-SRAM (APU) approach against traditional GPU acceleration. Energy efficiency is critical for sustainable AI deployment, especially for large-scale retrieval workloads.\n",
        "\n",
        "The experiment evaluates energy consumption across:\n",
        "- **Three corpus sizes**: 10GB, 50GB, 200GB\n",
        "- **Two platforms**: GPU vs. Compute-in-SRAM (APU)\n",
        "\n",
        "Energy breakdown components for APU:\n",
        "- **Static**: Base power consumption  \n",
        "- **DRAM**: Memory access energy (using theoretical HBM energy)\n",
        "- **L3,L2,L1**: Cache hierarchy energy\n",
        "- **Compute**: Processing unit energy\n",
        "- **Other**: Miscellaneous system energy\n",
        "\n",
        "The analysis:\n",
        "1. **Measures APU power consumption** using power profiling data\n",
        "2. **Calculates energy breakdown** for different RAG components  \n",
        "3. **Compares total energy consumption** between GPU and APU\n",
        "4. **Reports energy efficiency gains** from compute-in-SRAM approach\n",
        "\n",
        "**Output**:\n",
        "- Generates `energy_comparison.pdf` and `energy_comparison.png` with energy consumption comparison chart\n",
        "- Terminal table showing energy efficiency metrics (GPU energy / APU energy)\n",
        "- Detailed energy breakdown analysis\n",
        "- PNG figure displayed inline below after execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run RAG Energy Consumption Analysis\n",
        "run_experiment(\"5-rag-energy\",\n",
        "               \"RAG energy consumption comparison between GPU and compute-in-SRAM approaches\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Experiment 6: RAG Latency Breakdown Analysis\n",
        "\n",
        "This experiment provides detailed latency breakdown analysis for RAG (Retrieval-Augmented Generation) workloads, decomposing the total execution time into individual components. This analysis helps identify performance bottlenecks and optimization opportunities.\n",
        "\n",
        "The experiment analyzes latency breakdown for:\n",
        "- **Two optimization levels**: No optimization vs. All optimizations\n",
        "- **Three corpus sizes**: 10GB, 50GB, 200GB\n",
        "- **Five RAG components**:\n",
        "\n",
        "**RAG Pipeline Components:**\n",
        "1. **Load Embedding**: Loading document embeddings from HBM memory\n",
        "2. **Load Query**: Loading query vectors for similarity computation  \n",
        "3. **Calc Distance**: Computing similarity/distance between query and document embeddings\n",
        "4. **Top-K Aggregation**: Finding and aggregating the K most similar documents\n",
        "5. **Return Top-K**: Transferring the top-K results back to the host\n",
        "\n",
        "**Analysis Details:**\n",
        "- Uses measured execution times from Experiment 4 (RAG E2E)\n",
        "- Incorporates HBM memory access times based on theoretical models\n",
        "- Compares optimized vs. unoptimized implementations\n",
        "- Reports latencies in appropriate units (milliseconds for major components, microseconds for smaller ones)\n",
        "\n",
        "**Output**:\n",
        "- Terminal table showing detailed latency breakdown for all configurations\n",
        "- Comparison between optimization levels across different corpus sizes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run RAG Latency Breakdown Analysis  \n",
        "run_experiment(\"6-rag-latency-breakdown\",\n",
        "               \"Detailed latency breakdown analysis for RAG pipeline components\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
